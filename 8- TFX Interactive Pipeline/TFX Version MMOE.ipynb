{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TFX Version MMOE.ipynb","private_outputs":true,"provenance":[{"file_id":"1f1kuoH9v1uHGduw8mMUkIKc9JNzJmAih","timestamp":1607319035181}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"tc2LZmOn1oL1"},"source":[" try:\n","  import colab\n","  !pip install --upgrade pip\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"et4-4FjpITzm"},"source":["### Restart RUNTIME"]},{"cell_type":"code","metadata":{"id":"fOjkL5Wql6zN"},"source":["# !pip install transformers\n","# !pip install -q -U tfx\n","!pip install -q -U --use-feature=2020-resolver tfx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oASQ0Tr6M2zm"},"source":["### Import Libraries"]},{"cell_type":"code","metadata":{"id":"q2h1uKQJN4zD"},"source":["import os\n","import pprint\n","import tempfile\n","import urllib\n","\n","import absl\n","import tensorflow as tf\n","import tensorflow_model_analysis as tfma\n","tf.get_logger().propagate = False\n","pp = pprint.PrettyPrinter()\n","\n","import tfx\n","from tfx.components import CsvExampleGen\n","from tfx.components import Evaluator\n","from tfx.components import ExampleValidator\n","from tfx.components import Pusher\n","from tfx.components import ResolverNode\n","from tfx.components import SchemaGen\n","from tfx.components import StatisticsGen\n","from tfx.components import Trainer\n","from tfx.components import Transform\n","from tfx.components.base import executor_spec\n","from tfx.components.trainer.executor import GenericExecutor\n","from tfx.dsl.experimental import latest_blessed_model_resolver\n","from tfx.orchestration import metadata\n","from tfx.orchestration import pipeline\n","from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n","from tfx.proto import pusher_pb2\n","from tfx.proto import trainer_pb2\n","from tfx.types import Channel\n","from tfx.types.standard_artifacts import Model\n","from tfx.types.standard_artifacts import ModelBlessing\n","from tfx.utils.dsl_utils import external_input\n","# from tfx.utils.dsl_utils import csv_input\n","\n","%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1e-ythlnCyVu"},"source":["print('TensorFlow version: {}'.format(tf.__version__))\n","print('TFX version: {}'.format(tfx.__version__))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Fm4Td38lfbo"},"source":["# # This is the root directory for your TFX pip package installation.\r\n","_tfx_root = tfx.__path__[0]\r\n","\r\n","# This is the directory containing the YouTube Videos statistics Pipeline example.\r\n","_videos_root = os.path.join(_tfx_root, 'examples/Youtube_videos')\r\n","\r\n","# This is the path where your model will be pushed for serving.\r\n","_serving_model_dir = os.path.join(\r\n","    tempfile.mkdtemp(), 'serving_model/videos_simple')\r\n","\r\n","# Set up logging.\r\n","absl.logging.set_verbosity(absl.logging.INFO)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIB68hZBhomZ"},"source":["_data_root = tempfile.mkdtemp(prefix='tfx-data')\n","# DATA_PATH = 'https://raw.githubusercontent.com/varun-bhaseen/Dataset/master/Kaggle%20YouTube%20Statistics%20Dataset/videos.csv'\n","DATA_PATH = 'https://raw.githubusercontent.com/varun-bhaseen/Dataset/master/Kaggle%20YouTube%20Statistics%20Dataset/USvideos.csv'\n","_data_filepath = os.path.join(_data_root, \"videos.csv\")\n","urllib.request.urlretrieve(DATA_PATH, _data_filepath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGfzvv6giAx2"},"source":["!head {_data_filepath}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrwJU4FOiHjX"},"source":["context = InteractiveContext()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZeUGunZaiN-s"},"source":["### ExampleGen TFX"]},{"cell_type":"code","metadata":{"id":"CD_Wfw9PJpkm"},"source":["_data_root"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wj_xagyiiNJt"},"source":["# example_gen = CsvExampleGen(input=external_input(_data_root))\n","example_gen = CsvExampleGen(input_base=_data_root)\n","context.run(example_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9AdYmRgipfo"},"source":["artifact = example_gen.outputs['examples'].get()[0]\n","print(artifact.split_names, artifact.uri)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrJys3qMoZcP"},"source":["We can also take a look at the first three training examples:"]},{"cell_type":"code","metadata":{"id":"JQ1RcmefoUtT"},"source":["# Get the URI of the output artifact representing the training examples, which is a directory\n","train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n","\n","# Iterate over the first 3 records and decode them.\n","for tfrecord in dataset.take(3):\n","  serialized_example = tfrecord.numpy()\n","  example = tf.train.Example()\n","  example.ParseFromString(serialized_example)\n","  pp.pprint(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jS_zjigLoq7i"},"source":["### StatisticsGen\n","The `StatisticsGen` component computes statistics over your dataset for data analysis, as well as for use in downstream components. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n","\n","`StatisticsGen` takes as input the dataset we just ingested using `ExampleGen`."]},{"cell_type":"code","metadata":{"id":"Ra50iOAoot-l"},"source":["statistics_gen = StatisticsGen(\n","    examples=example_gen.outputs['examples'])\n","context.run(statistics_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myLrhy4NpPKJ"},"source":["After StatisticsGen finishes running, we can visualize the outputted statistics. Try playing with the different plots!"]},{"cell_type":"code","metadata":{"id":"5TcI8x5XpSAG"},"source":["context.show(statistics_gen.outputs['statistics'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uvm_PPTkpWJE"},"source":["### SchemaGen\n","\n","The `SchemaGen` component generates a schema based on your data statistics. (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n","\n","Note: The generated schema is best-effort and only tries to infer basic properties of the data. It is expected that you review and modify it as needed.\n","\n","`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."]},{"cell_type":"code","metadata":{"id":"Zk9SY6zepbnr"},"source":["schema_gen = SchemaGen(\n","    statistics=statistics_gen.outputs['statistics'],\n","    infer_feature_shape=False)\n","context.run(schema_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jM2wpiefpixF"},"source":["After SchemaGen finishes running, we can visualize the generated schema as a table."]},{"cell_type":"code","metadata":{"id":"diRogc2jpljm"},"source":["context.show(schema_gen.outputs['schema'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojUHzfXLprOA"},"source":["### ExampleValidator\n","The `ExampleValidator` component detects anomalies in your data, based on the expectations defined by the schema. It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n","\n","`ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`."]},{"cell_type":"code","metadata":{"id":"4BCkCuGmpvUQ"},"source":["example_validator = ExampleValidator(\n","    statistics=statistics_gen.outputs['statistics'],\n","    schema=schema_gen.outputs['schema'])\n","context.run(example_validator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15oyHMcLp1_e"},"source":["After ExampleValidator finishes running, we can visualize the anomalies as a table."]},{"cell_type":"code","metadata":{"id":"pkYbof0jpzht"},"source":["context.show(example_validator.outputs['anomalies'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AeUAEZxUp_Cr"},"source":["In the anomalies table, we can see that there are no anomalies. This is what we'd expect, since this the first dataset that we've analyzed and the schema is tailored to it. You should review this schema -- anything unexpected means an anomaly in the data. Once reviewed, the schema can be used to guard future data, and anomalies produced here can be used to debug model performance, understand how your data evolves over time, and identify data errors."]},{"cell_type":"markdown","metadata":{"id":"69TjiQTMqjjE"},"source":["### Transform\n","The `Transform` component performs feature engineering for both training and serving. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library.\n","\n","`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module that contains user-defined Transform code.\n","\n","Let's see an example of user-defined Transform code below (for an introduction to the TensorFlow Transform APIs, [see the tutorial](https://www.tensorflow.org/tfx/tutorials/transform/simple)). First, we define a few constants for feature engineering:\n","\n","Note: The `%%writefile` cell magic will save the contents of the cell as a `.py` file on disk. This allows the `Transform` component to load your code as a module.\n","\n"]},{"cell_type":"code","metadata":{"id":"ptZBJTJ7NzuX"},"source":["_videos_constants_module_file = 'videos_constants.py'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q82cwL-XN-Sv"},"source":["%%writefile {_videos_constants_module_file}\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBL1oHpzOB0V"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cz__wciUmRa2"},"source":["Prepare Data"]},{"cell_type":"code","metadata":{"id":"iszwcUKoxvdO"},"source":["def prepare_data(filename):\r\n","    \r\n","    df = pd.read_csv(filename)\r\n","    \r\n","    # get text embeddings\r\n","    print(\"generating bert embedding videos ....\")\r\n","    \r\n","    # combine all video info into one embeddings\r\n","    df['video_emb'] = df['title']\r\n","\r\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n","    model = TFBertModel.from_pretrained('bert-base-cased')\r\n","    max_length = 10 # change it to your computer capacity\r\n","    batch_encoding = tokenizer.batch_encode_plus(df['video_emb'].tolist(), max_length=max_length, pad_to_max_length=True)\r\n","\r\n","    outputs = model(tf.convert_to_tensor(batch_encoding['input_ids'])) # shape: (batch,sequence length, hidden state)\r\n","    \r\n","    embeddings_video = tf.reduce_mean(outputs[0],1)\r\n","    \r\n","    df['video_emb'] = embeddings_video.numpy().tolist()\r\n","  \r\n","    print(\"generating bert embedding for user...\")\r\n","    # assuming tags as user interested tags\r\n","    batch_encoding_user = tokenizer.batch_encode_plus(df['tags'].tolist(), max_length=max_length, pad_to_max_length=True)\r\n","    outputs_user = model(tf.convert_to_tensor(batch_encoding_user['input_ids'])) # shape: (batch,sequence length, hidden state)\r\n","    embeddings_user = tf.reduce_mean(outputs_user[0],1)\r\n","    df['user_emb'] = embeddings_user.numpy().tolist()\r\n","\r\n","    # min max scaling\r\n","    scaler = MinMaxScaler()\r\n","    df[['views', 'likes', 'dislikes','comment_total']] = scaler.fit_transform(df[['views', 'likes', 'dislikes','comment_total']])\r\n","\r\n","    # to speedup:\r\n","    df = df.reset_index(drop=True) #shuffle df\r\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OdP0iN1mGQf"},"source":["MMOE clas"]},{"cell_type":"code","metadata":{"id":"kjBV8dEJloSm"},"source":["class MMoE(Layer):\n","    \"\"\"\n","    Multi-gate Mixture-of-Experts model.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 units,\n","                 num_experts,\n","                 num_tasks,\n","                 use_expert_bias=True,\n","                 use_gate_bias=True,\n","                 expert_activation='relu',\n","                 gate_activation='softmax',\n","                 expert_bias_initializer='zeros',\n","                 gate_bias_initializer='zeros',\n","                 expert_bias_regularizer=None,\n","                 gate_bias_regularizer=None,\n","                 expert_bias_constraint=None,\n","                 gate_bias_constraint=None,\n","                 expert_kernel_initializer='VarianceScaling',\n","                 gate_kernel_initializer='VarianceScaling',\n","                 expert_kernel_regularizer=None,\n","                 gate_kernel_regularizer=None,\n","                 expert_kernel_constraint=None,\n","                 gate_kernel_constraint=None,\n","                 activity_regularizer=None,\n","                 **kwargs):\n","        \"\"\"\n","         Method for instantiating MMoE layer.\n","        :param units: Number of hidden units\n","        :param num_experts: Number of experts\n","        :param num_tasks: Number of tasks\n","        :param use_expert_bias: Boolean to indicate the usage of bias in the expert weights\n","        :param use_gate_bias: Boolean to indicate the usage of bias in the gate weights\n","        :param expert_activation: Activation function of the expert weights\n","        :param gate_activation: Activation function of the gate weights\n","        :param expert_bias_initializer: Initializer for the expert bias\n","        :param gate_bias_initializer: Initializer for the gate bias\n","        :param expert_bias_regularizer: Regularizer for the expert bias\n","        :param gate_bias_regularizer: Regularizer for the gate bias\n","        :param expert_bias_constraint: Constraint for the expert bias\n","        :param gate_bias_constraint: Constraint for the gate bias\n","        :param expert_kernel_initializer: Initializer for the expert weights\n","        :param gate_kernel_initializer: Initializer for the gate weights\n","        :param expert_kernel_regularizer: Regularizer for the expert weights\n","        :param gate_kernel_regularizer: Regularizer for the gate weights\n","        :param expert_kernel_constraint: Constraint for the expert weights\n","        :param gate_kernel_constraint: Constraint for the gate weights\n","        :param activity_regularizer: Regularizer for the activity\n","        :param kwargs: Additional keyword arguments for the Layer class\n","        \"\"\"\n","        # Hidden nodes parameter\n","        self.units = units\n","        self.num_experts = num_experts\n","        self.num_tasks = num_tasks\n","\n","        # Weight parameter\n","        self.expert_kernels = None\n","        self.gate_kernels = None\n","        self.expert_kernel_initializer = initializers.get(expert_kernel_initializer)\n","        self.gate_kernel_initializer = initializers.get(gate_kernel_initializer)\n","        self.expert_kernel_regularizer = regularizers.get(expert_kernel_regularizer)\n","        self.gate_kernel_regularizer = regularizers.get(gate_kernel_regularizer)\n","        self.expert_kernel_constraint = constraints.get(expert_kernel_constraint)\n","        self.gate_kernel_constraint = constraints.get(gate_kernel_constraint)\n","\n","        # Activation parameter\n","        self.expert_activation = activations.get(expert_activation)\n","        self.gate_activation = activations.get(gate_activation)\n","\n","        # Bias parameter\n","        self.expert_bias = None\n","        self.gate_bias = None\n","        self.use_expert_bias = use_expert_bias\n","        self.use_gate_bias = use_gate_bias\n","        self.expert_bias_initializer = initializers.get(expert_bias_initializer)\n","        self.gate_bias_initializer = initializers.get(gate_bias_initializer)\n","        self.expert_bias_regularizer = regularizers.get(expert_bias_regularizer)\n","        self.gate_bias_regularizer = regularizers.get(gate_bias_regularizer)\n","        self.expert_bias_constraint = constraints.get(expert_bias_constraint)\n","        self.gate_bias_constraint = constraints.get(gate_bias_constraint)\n","\n","        # Activity parameter\n","        self.activity_regularizer = regularizers.get(activity_regularizer)\n","\n","        # Keras parameter\n","        self.input_spec = InputSpec(min_ndim=2)\n","        self.supports_masking = True\n","\n","        super(MMoE, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        \"\"\"\n","        Method for creating the layer weights.\n","        :param input_shape: Keras tensor (future input to layer)\n","                            or list/tuple of Keras tensors to reference\n","                            for weight shape computations\n","        \"\"\"\n","        assert input_shape is not None and len(input_shape) >= 2\n","\n","        input_dimension = input_shape[-1]\n","\n","        # Initialize expert weights (number of input features * number of units per expert * number of experts)\n","        self.expert_kernels = self.add_weight(\n","            name='expert_kernel',\n","            shape=(input_dimension, self.units, self.num_experts),\n","            initializer=self.expert_kernel_initializer,\n","            regularizer=self.expert_kernel_regularizer,\n","            constraint=self.expert_kernel_constraint,\n","        )\n","\n","        # Initialize expert bias (number of units per expert * number of experts)\n","        if self.use_expert_bias:\n","            self.expert_bias = self.add_weight(\n","                name='expert_bias',\n","                shape=(self.units, self.num_experts),\n","                initializer=self.expert_bias_initializer,\n","                regularizer=self.expert_bias_regularizer,\n","                constraint=self.expert_bias_constraint,\n","            )\n","\n","        # Initialize gate weights (number of input features * number of experts * number of tasks)\n","        self.gate_kernels = [self.add_weight(\n","            name='gate_kernel_task_{}'.format(i),\n","            shape=(input_dimension, self.num_experts),\n","            initializer=self.gate_kernel_initializer,\n","            regularizer=self.gate_kernel_regularizer,\n","            constraint=self.gate_kernel_constraint\n","        ) for i in range(self.num_tasks)]\n","\n","        # Initialize gate bias (number of experts * number of tasks)\n","        if self.use_gate_bias:\n","            self.gate_bias = [self.add_weight(\n","                name='gate_bias_task_{}'.format(i),\n","                shape=(self.num_experts,),\n","                initializer=self.gate_bias_initializer,\n","                regularizer=self.gate_bias_regularizer,\n","                constraint=self.gate_bias_constraint\n","            ) for i in range(self.num_tasks)]\n","\n","        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dimension})\n","\n","        super(MMoE, self).build(input_shape)\n","\n","    def call(self, inputs, **kwargs):\n","        \"\"\"\n","        Method for the forward function of the layer.\n","        :param inputs: Input tensor\n","        :param kwargs: Additional keyword arguments for the base method\n","        :return: A tensor\n","        \"\"\"\n","        gate_outputs = []\n","        final_outputs = []\n","\n","        # add a shared bottom layer (relu layer)\n","\n","        # f_{i}(x) = activation(W_{i} * x + b), where activation is ReLU according to the paper\n","        # expert_outputs = K.tf.tensordot(a=inputs, b=self.expert_kernels, axes=1)\n","        expert_outputs = tf.tensordot(a=inputs, b=self.expert_kernels, axes=1)\n","        # Add the bias term to the expert weights if necessary\n","        if self.use_expert_bias:\n","            expert_outputs = K.bias_add(x=expert_outputs, bias=self.expert_bias)\n","        expert_outputs = self.expert_activation(expert_outputs)\n","\n","        # g^{k}(x) = activation(W_{gk} * x + b), where activation is softmax according to the paper\n","        for index, gate_kernel in enumerate(self.gate_kernels):\n","            gate_output = K.dot(x=inputs, y=gate_kernel)\n","            # Add the bias term to the gate weights if necessary\n","            if self.use_gate_bias:\n","                gate_output = K.bias_add(x=gate_output, bias=self.gate_bias[index])\n","            gate_output = self.gate_activation(gate_output)\n","            gate_outputs.append(gate_output)\n","\n","        # f^{k}(x) = sum_{i=1}^{n}(g^{k}(x)_{i} * f_{i}(x))\n","        for gate_output in gate_outputs:\n","            expanded_gate_output = K.expand_dims(gate_output, axis=1)\n","            weighted_expert_output = expert_outputs * K.repeat_elements(expanded_gate_output, self.units, axis=1)\n","            final_outputs.append(K.sum(weighted_expert_output, axis=2))\n","\n","        return final_outputs\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\"\n","        Method for computing the output shape of the MMoE layer.\n","        :param input_shape: Shape tuple (tuple of integers)\n","        :return: List of input shape tuple where the size of the list is equal to the number of tasks\n","        \"\"\"\n","        assert input_shape is not None and len(input_shape) >= 2\n","\n","        output_shape = list(input_shape)\n","        output_shape[-1] = self.units\n","        output_shape = tuple(output_shape)\n","\n","        return [output_shape for _ in range(self.num_tasks)]\n","\n","    def get_config(self):\n","        \"\"\"\n","        Method for returning the configuration of the MMoE layer.\n","        :return: Config dictionary\n","        \"\"\"\n","        config = {\n","            'units': self.units,\n","            'num_experts': self.num_experts,\n","            'num_tasks': self.num_tasks,\n","            'use_expert_bias': self.use_expert_bias,\n","            'use_gate_bias': self.use_gate_bias,\n","            'expert_activation': activations.serialize(self.expert_activation),\n","            'gate_activation': activations.serialize(self.gate_activation),\n","            'expert_bias_initializer': initializers.serialize(self.expert_bias_initializer),\n","            'gate_bias_initializer': initializers.serialize(self.gate_bias_initializer),\n","            'expert_bias_regularizer': regularizers.serialize(self.expert_bias_regularizer),\n","            'gate_bias_regularizer': regularizers.serialize(self.gate_bias_regularizer),\n","            'expert_bias_constraint': constraints.serialize(self.expert_bias_constraint),\n","            'gate_bias_constraint': constraints.serialize(self.gate_bias_constraint),\n","            'expert_kernel_initializer': initializers.serialize(self.expert_kernel_initializer),\n","            'gate_kernel_initializer': initializers.serialize(self.gate_kernel_initializer),\n","            'expert_kernel_regularizer': regularizers.serialize(self.expert_kernel_regularizer),\n","            'gate_kernel_regularizer': regularizers.serialize(self.gate_kernel_regularizer),\n","            'expert_kernel_constraint': constraints.serialize(self.expert_kernel_constraint),\n","            'gate_kernel_constraint': constraints.serialize(self.gate_kernel_constraint),\n","            'activity_regularizer': regularizers.serialize(self.activity_regularizer)\n","        }\n","        base_config = super(MMoE, self).get_config()\n","\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VI9LMrtnnK-G"},"source":["# Simple callback to print out ROC-AUC, only for classification\n","class ROCCallback(Callback):\n","    def __init__(self, training_data, validation_data, test_data):\n","        self.train_X = training_data[0]\n","        self.train_Y = training_data[1]\n","        self.validation_X = validation_data[0]\n","        self.validation_Y = validation_data[1]\n","        self.test_X = test_data[0]\n","        self.test_Y = test_data[1]\n","\n","    def on_train_begin(self, logs={}):\n","        return\n","\n","    def on_train_end(self, logs={}):\n","        return\n","\n","    def on_epoch_begin(self, epoch, logs={}):\n","        return\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        train_prediction = self.model.predict(self.train_X)\n","        validation_prediction = self.model.predict(self.validation_X)\n","        test_prediction = self.model.predict(self.test_X)\n","\n","        # Iterate through each task and output their ROC-AUC across different datasets\n","        for index, output_name in enumerate(self.model.output_names):\n","            if (output_name == 'user_click') or (output_name == 'user_like'): # cassification is roc-auc score\n","                train_roc_auc = roc_auc_score(self.train_Y[index], np.squeeze(train_prediction[index]))\n","                validation_roc_auc = roc_auc_score(self.validation_Y[index], np.squeeze(validation_prediction[index]))\n","                test_roc_auc = roc_auc_score(self.test_Y[index], np.squeeze(test_prediction[index]))\n","                print(\n","                    'ROC-AUC-{}-Train: {} ROC-AUC-{}-Validation: {} ROC-AUC-{}-Test: {}'.format(\n","                        output_name, round(train_roc_auc, 4),\n","                        output_name, round(validation_roc_auc, 4),\n","                        output_name, round(test_roc_auc, 4)\n","                    )\n","                )\n","            elif (output_name == 'user_rating') or (output_name == 'time_spend'): # regression is explained variance\n","                train_roc_auc = explained_variance_score(self.train_Y[index], np.squeeze(train_prediction[index]))\n","                validation_roc_auc = explained_variance_score(self.validation_Y[index], np.squeeze(validation_prediction[index]))\n","                test_roc_auc = explained_variance_score(self.test_Y[index], np.squeeze(test_prediction[index]))\n","                print(\n","                    'explained-variance-score-{}-Train: {} explained-variance-score-{}-Validation: {} explained-variance-score-{}-Test: {}'.format(\n","                        output_name, round(train_roc_auc, 4),\n","                        output_name, round(validation_roc_auc, 4),\n","                        output_name, round(test_roc_auc, 4)\n","                    )\n","                )\n","\n","        return\n","\n","    def on_batch_begin(self, batch, logs={}):\n","        return\n","\n","    def on_batch_end(self, batch, logs={}):\n","        return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sInOOWfhTr0A"},"source":["def build_model():\n","    # Set up the input layer\n","    input_video_emb = Input(shape=(768,))\n","    input_likes = Input(shape=(1,))\n","    input_dislikes = Input(shape=(1,))\n","    input_comments = Input(shape=(1,))\n","    input_user_emb = Input(shape=(768,))\n","    input_views = Input(shape=(1,))\n","    #input = Concatenate()([input_video_emb,input_user_emb,input_other_features])\n","    input = Concatenate()([input_video_emb,input_user_emb,input_likes,input_dislikes,input_comments,input_views])\n","    \n","    input_layer = ReLU()(input)\n","\n","    # add the shared ReLu layer\n","    # Set up MMoE layer\n","    mmoe_layers = MMoE(\n","        units=4,\n","        num_experts=8,\n","        num_tasks=4\n","    )(input_layer)\n","\n","    output_layers = []\n","    print(\"Output is user_click, user_rating, user_like and time_spend...\")\n","    output_info = [(1, 'user_click'),(1,'user_rating'),(1,'user_like'),(1,'time_spend')]  # the rating is categorical or regression?\n","    output_activation = ['softmax','linear','softmax','linear'] # None (linear) activation for regression task; softmax for classification\n","\n","    # Build tower layer from MMoE layer\n","    for index, task_layer in enumerate(mmoe_layers):\n","        tower_layer = Dense(\n","            units=8,\n","            activation='relu',\n","            kernel_initializer=VarianceScaling())(task_layer)\n","        output_layer = Dense(\n","            units=output_info[index][0],\n","            name=output_info[index][1],\n","            activation=output_activation[index],\n","            kernel_initializer=VarianceScaling())(tower_layer)\n","        output_layers.append(output_layer)\n","\n","    # Compile model\n","    # model = Model(inputs=[input_video_tags,input_video_title,input_video_desp,input_video_view], outputs=output_layers)\n","    model = Model(inputs=[input_video_emb,input_user_emb,input_likes,input_dislikes,input_comments,input_views], outputs=output_layers)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NiMrM1YZHL4E"},"source":["### Trainer\r\n","The `Trainer` component will train a model that you define in TensorFlow. Default Trainer support Estimator API, to use Keras API, you need to specify [Generic Trainer](https://github.com/tensorflow/community/blob/master/rfcs/20200117-tfx-generic-trainer.md) by setup `custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)` in Trainer's contructor.\r\n","\r\n","`Trainer` takes as input the schema from `SchemaGen`, the transformed data and graph from `Transform`, training parameters, as well as a module that contains user-defined model code.\r\n","\r\n","Let's see an example of user-defined model code below (for an introduction to the TensorFlow Keras APIs, [see the tutorial](https://www.tensorflow.org/guide/keras)):"]},{"cell_type":"code","metadata":{"id":"K9d3KCa7m1R8"},"source":["#### ==================\n","## train the model\n","#### ==================\n","def train_ranking_model(df):\n","\n","    train, val_test = train_test_split(df, test_size=0.3)\n","    val, test = train_test_split(val_test, test_size=0.5)\n","\n","\n","    train_label = [train[col_name].values for col_name in ['user_click', 'user_rating', 'user_like', 'time_spend']]\n","    # Tensorflow - ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float)\n","    train_data = [np.asarray(np.squeeze(train[['video_emb']].values.tolist())).astype(np.float32),\n","                  np.asarray(np.squeeze(train[['user_emb']].values.tolist())).astype(np.float32),\n","                  train[['likes']].values,\n","                  train[['dislikes']].values,\n","                  train[['comment_total']].values,\n","                  train[['views']].values] # todo: user demographics, device, time, and location\n","                  # np.asarray(train[['view_count']].values.tolist()).astype(np.float32)]\n","\n","    validation_label = [val[col_name].values for col_name in ['user_click', 'user_rating', 'user_like', 'time_spend']]\n","    validation_data = [np.asarray(np.squeeze(val[['video_emb']].values.tolist())).astype(np.float32),\n","                       np.asarray(np.squeeze(val[['user_emb']].values.tolist())).astype(np.float32),\n","                        val[['likes']].values,\n","                        val[['dislikes']].values,\n","                        val[['comment_total']].values,\n","                        val[['views']].values]\n","                       \n","                  # np.asarray(val[['view_count']].values.tolist()).astype(np.float32)]\n","\n","    test_label = [test[col_name].values for col_name in ['user_click', 'user_rating', 'user_like', 'time_spend']]\n","    test_data = [np.asarray(np.squeeze(test[['video_emb']].values.tolist())).astype(np.float32),\n","                 np.asarray(np.squeeze(test[['user_emb']].values.tolist())).astype(np.float32),\n","                 test[['likes']].values,\n","                 test[['dislikes']].values,\n","                 test[['comment_total']].values,\n","                 test[['views']].values]\n","                  # np.asarray(test[['view_count']].values.tolist()).astype(np.float32)]\n","\n","    print('Training data shape = {}'.format(train.shape))\n","    print('Validation data shape = {}'.format(val.shape))\n","    print('Test data shape = {}'.format(test.shape))\n","\n","    model = build_model()\n","\n","    adam_optimizer = Adam()\n","    model.compile(\n","        loss={'user_click': 'binary_crossentropy', 'user_rating':'MSE','user_like': 'binary_crossentropy','time_spend':'MSE'},\n","        optimizer=adam_optimizer,\n","        metrics=['accuracy']\n","    )\n","\n","    # Print out model architecture summary\n","    model.summary()\n","\n","    # CALLBACKS for main model\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"mmoe.hdf5\", save_best_only=True)\n","    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","    reduce_lr_cb = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n","    tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n","   \n","    # Train the model,\n","    model.fit(\n","        x=train_data,\n","        y=train_label,\n","        validation_data=(validation_data, validation_label),\n","        callbacks=[\n","            ROCCallback(\n","                training_data=(train_data, train_label),\n","                validation_data=(validation_data, validation_label),\n","                test_data=(test_data, test_label)\n","                ),\n","                checkpoint_cb, reduce_lr_cb, early_stopping_cb, tensorboard_cb\n","        ],\n","        epochs=100\n","    )\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtrIrmHyn5A1"},"source":["def train_position_bias_model(df):\n","    ###  ============================\n","    ## adding a shallow side tower to learn selection biase\n","    # measure https://en.wikipedia.org/wiki/Propensity_score_matching\n","    #### =============================\n","    ## train the selection bias\n","    ## \"shallow tower\": input: item position; output: relevance (clicked or not);\n","\n","    pos_shallow_tower = tf.keras.Sequential(\n","        [\n","            tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n","            tf.keras.layers.Dropout(0.1),\n","            tf.keras.layers.Dense(32, activation='relu'),\n","            tf.keras.layers.Dense(1, activation='softmax')\n","        ]\n","    )\n","\n","    print(\"the network structure for position bias prediction:\",pos_shallow_tower.summary())\n","    # sgd_opt = tf.keras.optimizers.SGD(lr=0.001)\n","    pos_shallow_tower.compile(\n","        loss='binary_crossentropy',\n","        optimizer='adam',\n","        metrics=['accuracy']\n","    )\n","\n","    position_df = pd.get_dummies(df, columns=['device_info'], prefix='', prefix_sep='')[['android', 'ios', 'web','pos_bias','position']]\n","    assert position_df.isnull().any().any() == False\n","\n","    train_pos, val_pos = train_test_split(position_df, test_size=0.2)\n","    train_pos_data = train_pos.drop(columns={'pos_bias'})\n","    train_pos_label = train_pos[['pos_bias']]\n","\n","    validation_pos_data =  val_pos.drop(columns={'pos_bias'})\n","    validation_pos_label = val_pos[['pos_bias']]\n","\n","    # CALLBACKS for shallow tower\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"biase.hdf5\", save_best_only=True)\n","    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","    reduce_lr_cb = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n","    tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n","\n","    pos_shallow_tower.fit(\n","        x=train_pos_data,\n","        y=train_pos_label,\n","        validation_data=(validation_pos_data, validation_pos_label),\n","        epochs=20,\n","        callbacks=[checkpoint_cb, reduce_lr_cb, early_stopping_cb, tensorboard_cb],\n","    )\n","    return pos_shallow_tower"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uKiBpEZWn50U"},"source":["def final_score(weights_for_engagement, weights_for_satification, main_model, position_biase_model,test_main,test_position):\n","\n","    print(\"the manually set weights for user engagement is\", str(weights_for_engagement))\n","    print(\"the manually set weights for user satisfaction is\", str(weights_for_satification))\n","\n","    preds = main_model.predict(test_main)\n","    user_click = preds[0]\n","    user_rating = preds[1]\n","    user_like = preds[2]\n","    time_spend = preds[3]\n","\n","    preds_position = position_biase_model.predict(test_position)\n","    return weights_for_engagement*expit(user_click+time_spend+preds_position) + weights_for_satification*expit(user_rating+user_like)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsMFG0hMHUUr"},"source":["### Evaluator\r\n","The `Evaluator` component computes model performance metrics over the evaluation set. It uses the [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started) library. The `Evaluator` can also optionally validate that a newly trained model is better than the previous model. This is useful in a production pipeline setting where you may automatically train and validate a model every day. In this notebook, we only train one model, so the `Evaluator` automatically will label the model as \"good\". \r\n","\r\n","`Evaluator` will take as input the data from `ExampleGen`, the trained model from `Trainer`, and slicing configuration. The slicing configuration allows you to slice your metrics on feature values (e.g. how does your model perform on taxi trips that start at 8am versus 8pm?). See an example of this configuration below:"]},{"cell_type":"markdown","metadata":{"id":"HEqoRpyzpCK9"},"source":["Train model"]},{"cell_type":"code","metadata":{"id":"5M9ko3HxpED3"},"source":["df = prepare_data('videos.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BqBKOZ_pHjg"},"source":["# creating models\n","print(\"train the main model ...\")\n","main_model = train_ranking_model(df)\n","print(\"train the shallow tower for position bias...\")\n","position_biase_model = train_position_bias_model(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZ3IzYqBsHxW"},"source":["Create Candidate List for the query"]},{"cell_type":"code","metadata":{"id":"gbcWR_Tar9nM"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3tPbgFms2YC"},"source":["def get_index_from_title(title):\n","    return df[df.title == title][\"index\"].values[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVTEu-CrsaGK"},"source":["cv = CountVectorizer()\n","count_matrix = cv.fit_transform(df[\"tags\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bD8bYU9dsppK"},"source":["cosine_sim = cosine_similarity(count_matrix)\n","df['index'] = range(0, len(df) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5FfCuQttG0_"},"source":["Our query"]},{"cell_type":"code","metadata":{"id":"TrSNrxRdtABJ"},"source":["query = \"iPhone X + iPhone 8 Hands on!\"\n","movie_index = get_index_from_title(query)\n","print('Query = ', movie_index, query)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RVhtvjc-tbYA"},"source":["# get similar movies\n","similar_movies = list(enumerate(cosine_sim[movie_index]))\n","sorted_similar_movies = sorted(similar_movies, key=lambda x:x[1], reverse=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcnHyenCtiyS"},"source":["# return top 50 as our candidate list\n","sorted_similar = sorted_similar_movies[1:51]\n","movie_indices = [i[0] for i in sorted_similar]\n","topMovies = df.iloc[movie_indices]\n","topMovies.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEOdkfOAuz-Y"},"source":["Save Candidate list "]},{"cell_type":"code","metadata":{"id":"CYWTPkJdtyeg"},"source":["#topMovies.to_csv('CandidateList.csv') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HKn13n58HhUE"},"source":["### Pusher\r\n","The `Pusher` component is usually at the end of a TFX pipeline. It checks whether a model has passed validation, and if so, exports the model to `_serving_model_dir`."]},{"cell_type":"markdown","metadata":{"id":"kvv_BBfMvFCA"},"source":["Rank our Candidate List using MMOe"]},{"cell_type":"code","metadata":{"id":"-z5QW18su_wM"},"source":["from keras.models import load_model\n","print(\"ranking...\")\n","test_data =  topMovies       \n","test_main = [np.asarray(np.squeeze(test_data[['video_emb']].values.tolist())).astype(np.float32),\n","          np.asarray(np.squeeze(test_data[['user_emb']].values.tolist())).astype(np.float32),\n","          test_data[['likes']].values,\n","          test_data[['dislikes']].values,\n","          test_data[['comment_total']].values,\n","          test_data[['views']].values]\n","\n","test_position = pd.get_dummies(test_data, columns=['device_info'], prefix='', prefix_sep='')[['android', 'ios', 'web','position']]\n","print(\"The final score for test data is...\")\n","\n","# try to load saved models\n","position_biase = load_model('biase.hdf5')\n","mmoe = build_model()\n","mmoe.load_weights('mmoe.hdf5')\n","adam_optimizer = Adam()\n","mmoe.compile(\n","        loss={'user_click': 'binary_crossentropy', 'user_rating':'MSE','user_like': 'binary_crossentropy','time_spend':'MSE'},\n","        optimizer=adam_optimizer,\n","        metrics=['accuracy'])\n","final_score_list = final_score(0.2, 0.8, mmoe, position_biase,test_main,test_position)\n","#final_score_list = final_score(0.2, 0.8, main_model, position_biase_model,test_main,test_position)\n","print(\"Final score = \", final_score_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJoEuFxJvWhN"},"source":["# merging results from model.predict() with test dataset\n","test_data['final_score'] = final_score_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzvPO5vawnby"},"source":["#sort by final_score desc\n","final_set = test_data.sort_values(by='final_score', ascending=False)\n","#select top 5 based on final_score\n","final_set[['title','final_score', 'tags']].head(5) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HzVKkYIJKvOX"},"source":[""],"execution_count":null,"outputs":[]}]}